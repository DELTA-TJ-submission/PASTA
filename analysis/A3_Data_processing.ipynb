{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5fa005-3104-4a54-8fb0-9705b0e3e9db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A3 - Data preprocessing for training and benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd94d13-6765-4d9a-9848-460c8144c13c",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to prepare paired ST-image for both model training, benchmarking and inference with ST. If you only have H&E images and wish to run inference, please follow the steps outlined in the README file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c683d3-f1b5-4eac-aea1-2fe95849925a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scanpy as sc\n",
    "import os\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9c2ce-aa6c-494c-bb2a-c480f06797d4",
   "metadata": {},
   "source": [
    "The input data of PASTA is fully compatible with formats used in the [HEST](https://github.com/mahmoodlab/HEST) library. You can directly download the pre-processed data from HEST or adhere to its processing pipeline to prepare your own datasets. The data structure should be structured as follows:\n",
    "```\n",
    "data_train/\n",
    "├── meta.csv                    # Sample metadata\n",
    "├── patches/                    # H5 files with patches\n",
    "│   ├── sample001.h5\n",
    "│   ├── sample002.h5\n",
    "│   └── ...\n",
    "├── st/\n",
    "│   ├── sample001.h5ad          # ST h5ad files\n",
    "│   ├── sample002.h5ad \n",
    "│   └── ...\n",
    "├── gene/\n",
    "│   ├── sample001.csv           # Gene expression\n",
    "│   ├── sample002.csv \n",
    "│   └── ...\n",
    "├── pathway/\n",
    "│   ├── sample001.csv           # Pathway scores\n",
    "│   ├── sample002.csv \n",
    "│   └── ...\n",
    "├── wsis/\n",
    "│   ├── sample001.tif           # HE images\n",
    "│   ├── sample002.tif \n",
    "│   └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592504be-3454-4e14-92e0-5e60d8263543",
   "metadata": {},
   "source": [
    "#### (Optional) Extract auxiliary tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9ac31-dc65-4147-ba72-bec1a2159835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tile_h5_path = f'/workspace/data/HEST/patches/TENX156.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393199ce-a1e5-401a-b36f-5a9577df7e58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['barcode', 'coords', 'img']>\n",
      "x min:555, y min:12360, x max:24397, y max:36669\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(tile_h5_path, 'r') as f:\n",
    "    print(f.keys())\n",
    "    coords = f['coords'][:]\n",
    "    x_min, y_min = coords.min(axis=0)\n",
    "    x_max, y_max = coords.max(axis=0)\n",
    "    print(f'x min:{x_min}, y min:{y_min}, x max:{x_max}, y max:{y_max}')\n",
    "    \n",
    "    img_attrs = dict(f['img'].attrs)\n",
    "    if 'patch_size' in img_attrs.keys():\n",
    "        patch_size = img_attrs['patch_size']\n",
    "    elif 'patch_size_src' in img_attrs.keys():\n",
    "        patch_size = img_attrs['patch_size_src']\n",
    "    else:\n",
    "        print('No patch size attrs in h5 file, please set patch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580d0c8-73c0-48a5-a59b-59f1ba7a951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_new = []; imgs_new=[]\n",
    "step = patch_size//2\n",
    "for i in range(x_min, x_max+step, step):\n",
    "    for j in range(y_min, y_max+step, step):\n",
    "        coords_new.append([i,j])\n",
    "        patch_tmp = wsi.read_region((int(i-step),int(j-step)), 0, (patch_size, patch_size)).convert('RGB').resize((224, 224))\n",
    "        patch_tmp_np = np.array(patch_tmp)\n",
    "        imgs_new.append(patch_tmp_np)\n",
    "coords_new = np.array(coords_new)\n",
    "imgs_new = np.stack(imgs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03414166-c488-4d27-b25d-f3385ae5dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(f'/workspace/data/HEST/patches/TENX156_aux.h5', 'w') as f:\n",
    "    f.create_dataset('coords', data=coords_new)\n",
    "    f.create_dataset('img', data=imgs_new)\n",
    "    for key, value in img_attrs.items():\n",
    "        f['img'].attrs[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2362c-69af-44c9-8f35-a05f45ec7135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca4c1d61-2210-476c-8511-0eb957370a9a",
   "metadata": {},
   "source": [
    "#### Pathway scores file preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d50ddb-d551-41da-be82-02c1ef2064a4",
   "metadata": {},
   "source": [
    "If you wish to train a model focusing on specific pathways, you can refer to the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac351d4b-d9a6-4ac1-ab4c-9ba0b69e7b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install gseapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082df0e4-eeb4-46f6-9e8c-f7e9a773bb92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gseapy as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3321b4a1-0ef9-4f0c-a002-94d8e02e2cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gmt_path = '/workspace/pasta/configs/gmt/Tumor_gene_sets.gmt'\n",
    "gmt = gp.base.GSEAbase()\n",
    "gmt = gmt.parse_gmt(gmt_path)\n",
    "pathway_names = list(gmt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff58523-036c-478a-a7ed-9fa41be7aa77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ssgsea_pathway(exp: pd.DataFrame, gmt, pathway_name:list) -> pd.DataFrame:\n",
    "    \"\"\"Run ssGSEA and return pathway scores.\"\"\"\n",
    "    if 'Name' not in exp.columns:\n",
    "        exp.insert(0, 'Name', exp.index)\n",
    "    exp = exp.drop_duplicates()\n",
    "    ss = gp.ssgsea(data=exp,\n",
    "                   gene_sets=gmt,\n",
    "                   min_size=1,\n",
    "                   outdir=None,\n",
    "                   sample_norm_method='rank', # choose 'custom' will only use the raw value of `data`\n",
    "                   no_plot=True,\n",
    "                   processes=10)\n",
    "    ss_df = ss.res2d.pivot(index='Term', columns='Name', values='NES')\n",
    "    if exp.shape[1]>1:\n",
    "        ori_idx = exp.columns[1:]\n",
    "        return ss_df.loc[:,ori_idx]\n",
    "    if ss_df.shape[0]<len(pathway_name):\n",
    "        no_results = [col for col in pathway_name if col not in ss_df.columns]\n",
    "        print(f'No results of {no_results}')\n",
    "    return ss_df\n",
    "\n",
    "def calculate_pathways_score(exp: pd.DataFrame, pathway_name:list, gmt, agg=False, file_id=None):\n",
    "    '''Calculate curated pathways scores'''\n",
    "    pathway_df = ssgsea_pathway(exp, gmt, pathway_name)\n",
    "    pathway_df = pathway_df.astype(np.float32)\n",
    "    if agg:\n",
    "        map_dict = {'Cell_Cycle':['Cell_Cycle_G1/S', 'Cell_Cycle_G2/M']}\n",
    "\n",
    "        map_reverse_dict = {v: k for k, values in map_dict.items() for v in values}\n",
    "        \n",
    "        pathway_df = pathway_df.rename(index=map_reverse_dict).groupby(level=0).mean()\n",
    "    \n",
    "    return pathway_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d57934-678d-4333-893c-6dbcfa102b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['NCBI785']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3e50d-1858-4ba6-b290-d15b8567643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"/workspace/data/HEST/pathway/\", exist_ok=True)\n",
    "for sample in file_list:\n",
    "    adata = sc.read_h5ad(f'/workspace/data/HEST/st/{sample}.h5ad')\n",
    "    pathway_df = pd.DataFrame(0, index=adata.obs.index, columns=pathway_names)\n",
    "    pathway = calculate_pathways_score(adata.to_df().T, gmt, pathway_names)\n",
    "    pathway = pathway.astype(np.float32).T\n",
    "    pathway_df.to_csv(f'/workspace/data/HEST/pathway/{sample}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8b101-704b-42f6-af90-3df07f1097dc",
   "metadata": {},
   "source": [
    "You can set info_dir in train.py to either the gene/ or pathway/ directory, depending on whether you want to use gene expression or pathway scores as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf2c127-8ecc-4cc1-a9d9-b0c5d6de1808",
   "metadata": {},
   "source": [
    "#### Aggregate Xenium or Visium HD data into different granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b1cfb-9734-44ba-bd29-6d822372a84f",
   "metadata": {},
   "source": [
    "Functions for pooling [Xenium](https://github.com/mahmoodlab/HEST/blob/3c39afa2bd8d407db904e285021466292a2be09e/src/hest/readers.py#L1068) or [Visium HD](https://github.com/mahmoodlab/HEST/blob/3c39afa2bd8d407db904e285021466292a2be09e/src/hest/readers.py#L1160) data is accessible via the HEST. Here, we offer a basic implementation of that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374a074-6235-495f-8fc2-bf21a8ca531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hest\n",
    "from hest import iter_hest\n",
    "from hest.readers import XeniumReader\n",
    "from hest.readers import pool_transcripts_xenium,pool_bins_visiumhd\n",
    "from hest.utils import find_pixel_size_from_spot_coords\n",
    "\n",
    "file_list = ['NCBI785']\n",
    "tot = len(file_list)\n",
    "for idx, st in enumerate(iter_hest('/workspace/data/HEST/', id_list=file_list, load_transcripts=True)):\n",
    "    for spot_size in [16, 32, 64]:\n",
    "        adata_hd_um = pool_transcripts_xenium(st.transcript_df, pixel_size_he=st.meta[\"pixel_size_um_estimated\"], spot_size_um=spot_size)\n",
    "        os.makedirs(f\"/workspace/data/benchmark/st_agg/{spot_size}um/\", exist_ok=True)\n",
    "        adata_hd_um.write_h5ad(f\"/workspace/data/benchmark/st_agg/{spot_size}um/{file_list[idx]}.h5ad\")\n",
    "        print(f\"[{idx}/{tot}] Finish {file_list[idx]} of {spot_size} um spots!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85602868-c4bb-4aea-8103-654ff986e66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8a047-59aa-4eb8-b726-df6035a4687c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
